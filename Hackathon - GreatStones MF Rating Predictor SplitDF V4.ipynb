{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon - Predicting mutual fund rating\n",
    "\n",
    "## Problem Satement :\n",
    "Great Stone Rating is a star based ranking system. These ratings are based on the performance of a mutual fund with adjustments for risks and costs as compared to other funds in the same category. The rating ranges from 0 to 5.\n",
    "\n",
    "## Goal: \n",
    "The goal of this hackathon is to predict GreatStone’s rating of a mutual fund. In order to help investors decide on which  mutual fund to pick for an investment, the task is to build a model that can predict the rating of a mutual fund. The various attributes that define a mutual fund can be used for building the model.\n",
    "\n",
    "## Dataset Information :\n",
    "This dataset comprises information of 25000 mutual funds in the United states. Various attributes related to the mutual fund have been described and these attributes will be used for making decisions on the rating of the mutual fund by GreatStone which is a top mutual fund rating agency. The following files are provided in the form of CSVs. These files contain various\n",
    "attributes related to the mutual fund. Please find the following files for the same:\n",
    "bond_ratings, fund_allocations, fund_config, fund_ratios, fund_specs, other_specs, return_3year, return_5year, return_10year.\n",
    "\n",
    "## Files Description:\n",
    "bond_ratings consists of 12 columns which provide information on the bond rating percentage allocation of the mutual funds\n",
    "fund_allocations consists of 12 columns which provide information on thesector wise percentage allocation of the mutual funds\n",
    "fund_config comprises of 4 columns which comprise the metadata of the mutual funds\n",
    "fund_ratios consists of 8 columns which provides information on various fundamental ratios that describe the mutual funds\n",
    "fund_specs contains 9 columns which give information about the specifications of the mutual funds\n",
    "other_specs contains 43 columns which give information of the other aspects of the mutual funds\n",
    "return_3years contains 17 columns which give information about 3 year return and ratios\n",
    "return_5years contains 17 columns which give information about 5 year return and ratios\n",
    "return_10years contains 17 columns which give information about 10 year return and ratios\n",
    "\n",
    "sample_submission contains the fund ids for which you need to provide the ratings for the submission file. Please maintain the order of the fund ids as shown in this file. The tag column is a unique identifier and is also the same as the id.(i.e tag\n",
    "= id)\n",
    "\n",
    "## Train and Test Data\n",
    "The train and test data are both provided in the CSVs described above as part of the same file. You need to segregate the training and test data based on where the greatstone ratings are provided. Go through the files carefully to understand how you\n",
    "can segregate both the datas. Please maintain the ordering of the test data. You can use the sample submission file in order to get ID of the test data.\n",
    "\n",
    "## Evaluation Metric\n",
    "Mean Precision Value - Mean of precision of all the classes = P1+P2….. P6/6 Here P1 is Precision of Class 1 and P2 is Precision of Class2 and so on and so forth.\n",
    "\n",
    "## Input  & Output files\n",
    "Available along wityh this github folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Approach \n",
    "\n",
    "The below document details the approach taken to solve the problem of predicting the rating of mutual funds. \n",
    "\n",
    "### Design decision 1:  Split the dataset\n",
    "By the nature of the mutual fund industry, the handling of funds will vary depending on the type of funds & the risk factors involved. The debt-based funds will be of less risk while the equity based funds will be of higher market risk. Hence the ratings will be based on the fund type / category. Hence the 1st design decision is the split the entire dataset based on the ‘Investment Class’ [Value based investment, Growth & Blended].  When the investment class is empty (Null), create a separate class called ‘Unknown’.  \n",
    "### Design decision 2 : Imputations\n",
    "The data imputations for the missing data will be done within the data subset based on investment class. The data imputations will be done after the split of data subsets based on investment class. This will be due to the fact that the items under any class will be treated separately & hence it will be apt to do mean based imputations within a investment class. \n",
    "\n",
    "### Design decision 3 : Different algorithms for different investment classes\n",
    "Since the datasets will be separated based on investment class, each subset will be split into train & test. For each data subset, the train data will again be split into X_train, y_train, X_test, y_test (this is equivalent of train & validation data).   Multiple algorithms will be run on this train & validation data. Also for each model, we will use a RandomSearch Cross validation to get the optimal hyper parameters which give best results.  Final model will be chosen based on the best results among the different models (with hyper parameter tuned). Hence the final result will be a combination of models – different model for individual data subset – which will be merged to form the final result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "#from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "#from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "# Below two lines for gridsearch \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data frams from csv files\n",
    "df_other_specs = pd.read_csv(\"other_specs.csv\")\n",
    "df_fund_specs = pd.read_csv(\"fund_specs.csv\")\n",
    "df_fund_ratios = pd.read_csv(\"fund_ratios.csv\")\n",
    "df_submission_file = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "df_return_3Y = pd.read_csv(\"return_3year.csv\")\n",
    "df_return_5Y = pd.read_csv(\"return_5year.csv\")\n",
    "df_return_10Y = pd.read_csv(\"return_10year.csv\")\n",
    "\n",
    "df_fund_config = pd.read_csv(\"fund_config.csv\")\n",
    "\n",
    "#df_fund_allocations = pd.read_csv(\"fund_allocations.csv\")\n",
    "#df_fund_allocations.rename(columns={'id': 'tag'},inplace=True)\n",
    "## Above commenedt as this reduces precision\n",
    "\n",
    "df_bond_ratings = pd.read_csv(\"bond_ratings.csv\")\n",
    "## Above commenedt as this reduces precision\n",
    "\n",
    "\n",
    "## Others to be considered later are bond allocations for blend & value funds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with common linkage between fund_id & tag\n",
    "df_linkage = df_fund_ratios.filter(['fund_id', 'tag'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create features that may be useful in each df\n",
    "#1. Fund_Specs: None to be created\n",
    "df_fund_specs.drop(columns=['currency','total_assets', 'inception_date'], inplace=True)\n",
    "\n",
    "df_other_specs.drop(columns=['greatstone_rating'], inplace=True)\n",
    "\n",
    "## DF3 : Fund Ratios : \n",
    "df_fund_ratios.drop(columns=['mmc','fund_id','pb_ratio','ps_ratio','pc_ratio','pe_ratio'],inplace=True)\n",
    "\n",
    "### NEW FEATURE CREATION.  Create new feature(s) for ration between the individual value to the category value. \n",
    "\n",
    "#DF4 : 3 year return : Retain = fund_return_3years, 3_years_return_category\n",
    "df_return_3Y['3yrs_treynor_ratio_fund'] = pd.to_numeric(df_return_3Y['3yrs_treynor_ratio_fund'], downcast=\"float\",errors='coerce')\n",
    "\n",
    "df_return_3Y['3y_treynor_ratio'] = df_return_3Y['3yrs_treynor_ratio_fund']/df_return_3Y['3yrs_treynor_ratio_category']\n",
    "df_return_3Y['3y_treynor_ratio'] = df_return_3Y['3y_treynor_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_3Y['3y_alpha_ratio'] = df_return_3Y['3_years_alpha_fund']/df_return_3Y['3_years_alpha_category']\n",
    "df_return_3Y['3y_alpha_ratio'] = df_return_3Y['3y_alpha_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_3Y['3y_sharpe_ratio'] = df_return_3Y['3yrs_sharpe_ratio_fund']/df_return_3Y['3yrs_sharpe_ratio_category']\n",
    "df_return_3Y['3y_sharpe_ratio'] = df_return_3Y['3y_sharpe_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_3Y['3y_rma_ratio'] = df_return_3Y['3_years_return_mean_annual_fund']/df_return_3Y['3_years_return_mean_annual_category']\n",
    "df_return_3Y['3y_rma_ratio'] = df_return_3Y['3y_rma_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_3Y['3y_beta_ratio'] = df_return_3Y['fund_beta_3years']/df_return_3Y['category_beta_3years']\n",
    "df_return_3Y['3y_beta_ratio'] = df_return_3Y['3y_beta_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_3Y['3y_r2_ratio'] = df_return_3Y['3years_fund_r_squared']/df_return_3Y['3years_category_r_squared']\n",
    "df_return_3Y['3y_r2_ratio'] = df_return_3Y['3y_r2_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_3Y['3y_std_ratio'] = df_return_3Y['3years_fund_std']/df_return_3Y['3years_category_std']\n",
    "df_return_3Y['3y_std_ratio'] = df_return_3Y['3y_std_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_3Y['3y_return_ratio'] = df_return_3Y['fund_return_3years']/df_return_3Y['3_years_return_category']\n",
    "df_return_3Y['3y_return_ratio'] = df_return_3Y['3y_return_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "\n",
    "# Drop columns that are no more required as the new features are created from these \n",
    "df_return_3Y.drop(columns=[\n",
    "    '3yrs_treynor_ratio_fund','3yrs_treynor_ratio_category',\n",
    "    '3_years_alpha_fund','3_years_alpha_category',\n",
    "    '3yrs_sharpe_ratio_fund','3yrs_sharpe_ratio_category',\n",
    "    '3_years_return_mean_annual_fund','3_years_return_mean_annual_category',\n",
    "    'fund_beta_3years','category_beta_3years',\n",
    "    '3years_fund_r_squared','3years_category_r_squared',\n",
    "    '3years_fund_std','3years_category_std',\n",
    "    'fund_return_3years','3_years_return_category'],inplace=True)\n",
    "\n",
    "df_return_5Y['5yrs_treynor_ratio_fund'] = pd.to_numeric(df_return_5Y['5yrs_treynor_ratio_fund'], downcast=\"float\",errors='coerce')\n",
    "\n",
    "df_return_5Y['5y_treynor_ratio'] = df_return_5Y['5yrs_treynor_ratio_fund']/df_return_5Y['5yrs_treynor_ratio_category']\n",
    "df_return_5Y['5y_treynor_ratio'] = df_return_5Y['5y_treynor_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_5Y['5y_alpha_ratio'] = df_return_5Y['5_years_alpha_fund']/df_return_5Y['5_years_alpha_category']\n",
    "df_return_5Y['5y_alpha_ratio'] = df_return_5Y['5y_alpha_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_5Y['5y_sharpe_ratio'] = df_return_5Y['5yrs_sharpe_ratio_fund']/df_return_5Y['5yrs_sharpe_ratio_category']\n",
    "df_return_5Y['5y_sharpe_ratio'] = df_return_5Y['5y_sharpe_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_5Y['5y_rma_ratio'] = df_return_5Y['5_years_return_mean_annual_fund']/df_return_5Y['5_years_return_mean_annual_category']\n",
    "df_return_5Y['5y_rma_ratio'] = df_return_5Y['5y_rma_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_5Y['5y_beta_ratio'] = df_return_5Y['5_years_beta_fund']/df_return_5Y['5_years_beta_category']\n",
    "df_return_5Y['5y_beta_ratio'] = df_return_5Y['5y_beta_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_5Y['5y_r2_ratio'] = df_return_5Y['5years_fund_r_squared']/df_return_5Y['category_r_squared_5years']\n",
    "df_return_5Y['5y_r2_ratio'] = df_return_5Y['5y_r2_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_5Y['5y_std_ratio'] = df_return_5Y['5years_fund_std']/df_return_5Y['5years_category_std']\n",
    "df_return_5Y['5y_std_ratio'] = df_return_5Y['5y_std_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_5Y['5y_return_ratio'] = df_return_5Y['5_years_return_fund']/df_return_5Y['5_years_return_category']\n",
    "df_return_5Y['5y_return_ratio'] = df_return_5Y['5y_return_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "\n",
    "df_return_5Y.drop(columns=[\n",
    "    '5yrs_treynor_ratio_fund','5yrs_treynor_ratio_category',\n",
    "    '5_years_alpha_fund','5_years_alpha_category',\n",
    "    '5yrs_sharpe_ratio_fund','5yrs_sharpe_ratio_category',\n",
    "    '5_years_return_mean_annual_fund','5_years_return_mean_annual_category',\n",
    "    '5_years_beta_fund','5_years_beta_category',\n",
    "    '5years_fund_r_squared','category_r_squared_5years',\n",
    "    '5years_fund_std','5years_category_std',\n",
    "    '5_years_return_fund','5_years_return_category'],inplace=True)\n",
    "\n",
    "#DF6 : 10 year return : \n",
    "df_return_10Y['10yrs_treynor_ratio_fund'] = pd.to_numeric(df_return_10Y['10yrs_treynor_ratio_fund'], downcast=\"float\",errors='coerce')\n",
    "\n",
    "df_return_10Y['10y_treynor_ratio'] = df_return_10Y['10yrs_treynor_ratio_fund']/df_return_10Y['10yrs_treynor_ratio_category']\n",
    "df_return_10Y['10y_treynor_ratio'] = df_return_10Y['10y_treynor_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_10Y['10y_alpha_ratio'] = df_return_10Y['10_years_alpha_fund']/df_return_10Y['10_years_alpha_category']\n",
    "df_return_10Y['10y_alpha_ratio'] = df_return_10Y['10y_alpha_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_10Y['10y_sharpe_ratio'] = df_return_10Y['10yrs_sharpe_ratio_fund']/df_return_10Y['10yrs_sharpe_ratio_category']\n",
    "df_return_10Y['10y_sharpe_ratio'] = df_return_10Y['10y_sharpe_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_10Y['10y_rma_ratio'] = df_return_10Y['10_years_return_mean_annual_fund']/df_return_10Y['10_years_return_mean_annual_category']\n",
    "df_return_10Y['10y_rma_ratio'] = df_return_10Y['10y_rma_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_10Y['10y_beta_ratio'] = df_return_10Y['10_years_beta_fund']/df_return_10Y['10_years_beta_category']\n",
    "df_return_10Y['10y_beta_ratio'] = df_return_10Y['10y_beta_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_10Y['10y_r2_ratio'] = df_return_10Y['10years_fund_r_squared']/df_return_10Y['10years_category_r_squared']\n",
    "df_return_10Y['10y_r2_ratio'] = df_return_10Y['10y_r2_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_10Y['10y_std_ratio'] = df_return_10Y['10years_fund_std']/df_return_10Y['10years_category_std']\n",
    "df_return_10Y['10y_std_ratio'] = df_return_10Y['10y_std_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "df_return_10Y['10y_return_ratio'] = df_return_10Y['10_years_return_fund']/df_return_10Y['10_years_return_category']\n",
    "df_return_10Y['10y_return_ratio'] = df_return_10Y['10y_return_ratio'].replace((np.inf, -np.inf), (0, 0))\n",
    "\n",
    "df_return_10Y.drop(columns=[\n",
    "    '10yrs_treynor_ratio_fund','10yrs_treynor_ratio_category',\n",
    "    '10_years_alpha_fund','10_years_alpha_category',\n",
    "    '10yrs_sharpe_ratio_fund','10yrs_sharpe_ratio_category',\n",
    "    '10_years_return_mean_annual_fund','10_years_return_mean_annual_category',\n",
    "    '10_years_beta_fund','10_years_beta_category',\n",
    "    '10years_fund_r_squared','10years_category_r_squared',\n",
    "    '10years_fund_std','10years_category_std',\n",
    "    '10_years_return_fund','10_years_return_category'],inplace=True)\n",
    "\n",
    "## FEATURE ENGINEERIGN : create a differential weighted value for for each bond rating.  \n",
    "df_bond_ratings['aaa_rating'] = df_bond_ratings['aaa_rating'] * 4\n",
    "df_bond_ratings['aa_rating'] = df_bond_ratings['aa_rating'] * 3\n",
    "df_bond_ratings['a_rating'] = df_bond_ratings['a_rating'] * 2\n",
    "df_bond_ratings['bbb_rating'] = df_bond_ratings['bbb_rating'] * 1\n",
    "df_bond_ratings['bb_rating'] = df_bond_ratings['bb_rating'] * 0.5\n",
    "df_bond_ratings['b_rating'] = df_bond_ratings['b_rating'] * -1\n",
    "df_bond_ratings['below_b_rating'] = df_bond_ratings['below_b_rating'] * -3\n",
    "df_bond_ratings['others_rating'] = df_bond_ratings['others_rating'] * -3\n",
    "\n",
    "\n",
    "df_bond_ratings.drop(columns=['us_govt_bond_rating'],inplace=True)\n",
    "\n",
    "\n",
    "df_fund_config.drop(columns=['parent_company','fund_name'],inplace=True)\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df_fund_config['category']= label_encoder.fit_transform(df_fund_config['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_ny = ['tag','fund_id']\n",
    "for i in df_return_3Y.columns:\n",
    "    if i not in exclude_ny:\n",
    "        df_return_3Y[i] = pd.to_numeric(df_return_3Y[i], downcast=\"float\",errors='coerce')\n",
    "for i in df_return_5Y.columns:\n",
    "    if i not in exclude_ny:\n",
    "        df_return_5Y[i] = pd.to_numeric(df_return_5Y[i], downcast=\"float\",errors='coerce')\n",
    "\n",
    "for i in df_return_10Y.columns:\n",
    "    if i not in exclude_ny:\n",
    "        df_return_10Y[i] = pd.to_numeric(df_return_10Y[i], downcast=\"float\",errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create master data\n",
    "# DF1 : Linkage. Drop no columns\n",
    "# DF2 : Fund_Specs \n",
    "df_dataset = pd.merge(df_linkage,df_fund_specs,on='tag')\n",
    "\n",
    "#DF3 : other_specs \n",
    "df_dataset = pd.merge(df_dataset,df_other_specs,on='tag')\n",
    "\n",
    "# DF4 : Fund ratios : \n",
    "df_dataset = pd.merge(df_dataset,df_fund_ratios,on='tag')\n",
    "\n",
    "#DF5 : 3 year return :\n",
    "#df_return_3Y = df_return_3Y.filter(['tag','fund_return_3years', '3_years_return_category'],axis=1)\n",
    "df_dataset = pd.merge(df_dataset,df_return_3Y,on='tag')\n",
    "\n",
    "#DF6 : 5 Year return\n",
    "df_dataset = pd.merge(df_dataset,df_return_5Y,on='tag')\n",
    "\n",
    "#DF7 : 10 Year return\n",
    "df_dataset = pd.merge(df_dataset,df_return_10Y,on='fund_id')\n",
    "\n",
    "# DF8 : Category from fund_category\n",
    "df_dataset = pd.merge(df_dataset,df_fund_config,on='fund_id')\n",
    "\n",
    "# DF9 : Fund Allocations\n",
    "#df_dataset = pd.merge(df_dataset,df_fund_allocations,on='tag')\n",
    "\n",
    "# DF10 :  Bond Ratings\n",
    "df_dataset = pd.merge(df_dataset,df_bond_ratings,on='tag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columsn as object are converted to float\n",
    "df_dataset['pc_ratio'] = pd.to_numeric(df_dataset['pc_ratio'], downcast=\"float\",errors='coerce')\n",
    "df_dataset['pb_ratio'] = pd.to_numeric(df_dataset['pb_ratio'], downcast=\"float\",errors='coerce')\n",
    "df_dataset['ps_ratio'] = pd.to_numeric(df_dataset['ps_ratio'], downcast=\"float\",errors='coerce')\n",
    "df_dataset['pe_ratio'] = pd.to_numeric(df_dataset['pe_ratio'], downcast=\"float\",errors='coerce')\n",
    "df_dataset['mmc'] = pd.to_numeric(df_dataset['mmc'], downcast=\"float\",errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute blanks in investment_class with 'Unkowwn'\n",
    "df_dataset['investment_class'] = df_dataset['investment_class'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute blanks in fund_size with 'Unkowwn'\n",
    "df_dataset['fund_size'] = df_dataset['fund_size'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode fund size\n",
    "fund_size = pd.get_dummies(df_dataset['fund_size'],drop_first=True)\n",
    "df_dataset.drop(['fund_size'],axis=1,inplace=True)\n",
    "df_dataset = pd.concat([df_dataset,fund_size],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing yield with zero\n",
    "df_dataset['yield'] = df_dataset['yield'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ['fund_id','tag','investment_class','greatstone_rating'] #,'fund_size']\n",
    "for i in df_dataset.columns:\n",
    "    if i not in exclude :\n",
    "        df_dataset[i] = round(pd.to_numeric(df_dataset[i]),3)\n",
    "        df_dataset[i] = df_dataset[i].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Value = df_dataset[df_dataset['investment_class']=='Value']\n",
    "df_Blend = df_dataset[df_dataset['investment_class']=='Blend']\n",
    "df_Growth = df_dataset[df_dataset['investment_class']=='Growth']\n",
    "df_Unknown = df_dataset[df_dataset['investment_class']=='Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# For each data subset, impute with mean of the columns within the data subset  (Design decision #2)\n",
    "\n",
    "data_subsets = [df_Value,df_Blend,df_Growth,df_Unknown]\n",
    "for df in data_subsets:\n",
    "    exclude = ['fund_id','tag','investment_class','yield','greatstone_rating'] #,'fund_size']\n",
    "    for i in df.columns:\n",
    "        if i not in exclude :            \n",
    "            df[i] = pd.to_numeric(df[i])\n",
    "            df[i] = df[i].fillna(df[i].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation table (CSV) to be inspected outside. Conditional formatting can be used to find correlartion.\n",
    "# A pictorical vire like SNS pairplot id avoided here as the data will become unreadable with too many columns\n",
    "\n",
    "df_corr = df_dataset.corr()\n",
    "df_corr.to_csv(\"correlations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Below code is for data subset where Investment class is VALUE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "## Fulll set of code for Investment_calss = VALUE\n",
    "df_Value_train = df_Value[df_Value.greatstone_rating.notnull()]\n",
    "df_Value_test = df_Value[df_Value.greatstone_rating.isnull()]\n",
    "\n",
    "df_linkage = df_dataset.filter(['fund_id', 'tag'],axis=1)\n",
    "df_linkage_Value = df_Value.filter(['fund_id', 'tag'],axis=1)\n",
    "df_Value_train_keys = df_Value_train.filter(['tag'],axis=1)\n",
    "df_Value_test_keys = df_Value_test.filter(['tag'],axis=1)\n",
    "\n",
    "keys_train = list(df_Value_train_keys.columns.values)\n",
    "i1 = df_linkage_Value.set_index(keys_train).index\n",
    "i2 = df_Value_train_keys.set_index(keys_train).index\n",
    "df_Value_linkage_train = df_linkage_Value[i1.isin(i2)]\n",
    "df_Value_linkage_test = df_linkage_Value[~i1.isin(i2)]\n",
    "\n",
    "# FEATURE ENGINEERING : Create new feature based on years up & years down. \n",
    "df_Value_train['yrs_Up_Down'] =df_Value_train['years_up']-df_Value_train['years_down']\n",
    "df_Value_train.drop(columns=['years_up','years_down'],inplace=True)\n",
    "df_Value_test['yrs_Up_Down'] =df_Value_test['years_up']-df_Value_test['years_down']\n",
    "df_Value_test.drop(columns=['years_up','years_down'],inplace=True)\n",
    "\n",
    "df_Value_train_X = df_Value_train.drop(labels='greatstone_rating',axis=1)\n",
    "df_Value_train_y = df_Value_train['greatstone_rating']\n",
    "\n",
    "## Below code for creatign train-test out of Value_Train\n",
    "VX_train, VX_test, Vy_train, Vy_test = train_test_split(df_Value_train_X,df_Value_train_y,test_size=0.2, random_state=108)\n",
    "\n",
    "# Drop funid & tag columns in both test & train \n",
    "df_Value_train_X.drop(columns=['fund_id','tag'],inplace=True)\n",
    "df_Value_test_y = df_Value_test['greatstone_rating']\n",
    "df_Value_test.drop(columns=['fund_id','tag','greatstone_rating'],inplace=True)\n",
    "\n",
    "\n",
    "# OneHot encode investment class in both train_X & test\n",
    "inv_class = pd.get_dummies(df_Value_train_X['investment_class'],drop_first=True)\n",
    "df_Value_train_X.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Value_train_X = pd.concat([df_Value_train_X,inv_class],axis=1)\n",
    "\n",
    "inv_class = pd.get_dummies(df_Value_test['investment_class'],drop_first=True)\n",
    "df_Value_test.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Value_test = pd.concat([df_Value_test,inv_class],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temp Model for Value : (RAMDOM FOREST)\n",
    "VX_train.drop(columns=['fund_id','tag','investment_class'],inplace=True)\n",
    "VX_test.drop(columns=['fund_id','tag','investment_class'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Note : The hyper parameters belowin each of the models are a result of RandomSearch CV after multiple iterations.  The code for RamdonSearch CV is available somewhere below , with being commented as this codel doesnt need to be run every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL 1 : RamdomForest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97        39\n",
      "         1.0       0.88      0.64      0.74        83\n",
      "         2.0       0.81      0.79      0.80       264\n",
      "         3.0       0.78      0.86      0.82       364\n",
      "         4.0       0.81      0.81      0.81       241\n",
      "         5.0       0.77      0.68      0.72        59\n",
      "\n",
      "    accuracy                           0.81      1050\n",
      "   macro avg       0.84      0.79      0.81      1050\n",
      "weighted avg       0.81      0.81      0.81      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 38,   1,   0,   0,   0,   0],\n",
       "       [  0,  53,  28,   2,   0,   0],\n",
       "       [  0,   6, 209,  48,   1,   0],\n",
       "       [  1,   0,  21, 314,  27,   1],\n",
       "       [  0,   0,   0,  35, 195,  11],\n",
       "       [  0,   0,   0,   2,  17,  40]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=1000, max_depth=50, min_samples_split=5, min_samples_leaf=2, max_features = 'auto', bootstrap = False, random_state= 108)\n",
    "rf_model.fit(VX_train,Vy_train)\n",
    "predictions = rf_model.predict(VX_test)\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Vy_test, predictions)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Vy_test, predictions))\n",
    "\n",
    "metrics.confusion_matrix(Vy_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Below is the RansdomSerach CV code for RandonForest  algo. This code is used only when during hyper parameter tuning . Whenever needed, uncomment the code & can be sued, Same code can be reused for all data subsets by passing the right train & test values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 500, stop = 5000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt','log2']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "# print(random_grid)\n",
    "\n",
    "\n",
    "# rf_model = RandomForestClassifier()\n",
    "\n",
    "# rf_random = RandomizedSearchCV(estimator = rf_model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=108, n_jobs = -1)\n",
    "\n",
    "# rf_random.fit(VX_train,Vy_train)\n",
    "\n",
    "# print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL 2 : XGBOOST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99        39\n",
      "         1.0       0.84      0.65      0.73        83\n",
      "         2.0       0.78      0.78      0.78       264\n",
      "         3.0       0.80      0.84      0.82       364\n",
      "         4.0       0.82      0.84      0.83       241\n",
      "         5.0       0.75      0.71      0.73        59\n",
      "\n",
      "    accuracy                           0.81      1050\n",
      "   macro avg       0.83      0.80      0.81      1050\n",
      "weighted avg       0.81      0.81      0.81      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 39,   0,   0,   0,   0,   0],\n",
       "       [  0,  54,  28,   1,   0,   0],\n",
       "       [  0,  10, 206,  47,   1,   0],\n",
       "       [  1,   0,  30, 304,  28,   1],\n",
       "       [  0,   0,   0,  25, 203,  13],\n",
       "       [  0,   0,   0,   2,  15,  42]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XG Boost\n",
    "D_train = xgb.DMatrix(VX_train, label=Vy_train)\n",
    "D_test = xgb.DMatrix(VX_test, label=Vy_test)\n",
    "\n",
    "param = {\n",
    "    'eta': 0.01, \n",
    "    'max_depth': 9,\n",
    "    'gamma' : 0.5,\n",
    "    'min_child_weight' :10 ,\n",
    "    'subsample' : 1.0,\n",
    "    'random_state': 108,\n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 6} \n",
    "\n",
    "steps = 3000  # The number of training iterations\n",
    "\n",
    "xgb_model = xgb.train(param, D_train, steps)\n",
    "predictions = xgb_model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Vy_test, best_preds)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Vy_test, best_preds))\n",
    "\n",
    "metrics.confusion_matrix(Vy_test, best_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Below is the RansdomSerach CV code for XGBoost algo. This code is used only when during hyper parameter tuning . Whenever needed, uncomment the code & can be sued, Same code can be reused for all data subsets by passing the right train & test values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP\n",
    "# ##### DO A RAMDOM SEARCH CV\n",
    "\n",
    "# # A parameter grid for XGBoost\n",
    "# params = {\n",
    "#         'min_child_weight': [2,3,4,5],\n",
    "#         'gamma': [ 1.5, 1.75,2,2.25],\n",
    "#         'subsample': [0.8,1.0],\n",
    "#         'colsample_bytree': [0.5,0.6,0.7, 0.8],\n",
    "#         'max_depth': [7,8,9,10]\n",
    "#         }\n",
    "\n",
    "# xgb = XGBClassifier(learning_rate=0.01, n_estimators=3000, objective='multi:softmax',\n",
    "#                     silent=True, nthread=1)\n",
    "\n",
    "# folds = 4\n",
    "# param_comb = 5\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 108)\n",
    "\n",
    "# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy',\n",
    "#                                    n_jobs=4, cv=skf.split(UX_train,Uy_train), verbose=3, random_state=108 )\n",
    "\n",
    "# random_search.fit(UX_train,Uy_train)\n",
    "\n",
    "# print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL 3 : XGBOOST Classifier \n",
    "Use same hyper parameters of XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99        39\n",
      "         1.0       0.85      0.64      0.73        83\n",
      "         2.0       0.79      0.78      0.79       264\n",
      "         3.0       0.80      0.85      0.83       364\n",
      "         4.0       0.83      0.86      0.84       241\n",
      "         5.0       0.81      0.71      0.76        59\n",
      "\n",
      "    accuracy                           0.82      1050\n",
      "   macro avg       0.84      0.81      0.82      1050\n",
      "weighted avg       0.82      0.82      0.81      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 39,   0,   0,   0,   0,   0],\n",
       "       [  0,  53,  29,   1,   0,   0],\n",
       "       [  0,   9, 207,  47,   1,   0],\n",
       "       [  1,   0,  26, 309,  27,   1],\n",
       "       [  0,   0,   0,  25, 207,   9],\n",
       "       [  0,   0,   0,   2,  15,  42]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1=XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, gamma=0.5, \n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=9,\n",
    "              min_child_weight=10, monotone_constraints=None,\n",
    "              n_estimators=3000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              objective='multi:softmax', random_state=108, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=None, silent=True, subsample=1.0,\n",
    "              tree_method=None, validate_parameters=False, verbosity=None)\n",
    "xgb1.fit(VX_train,Vy_train)\n",
    "predictions = xgb1.predict(VX_test)\n",
    "#best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Vy_test, predictions)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Vy_test, predictions))\n",
    "\n",
    "metrics.confusion_matrix(Vy_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Below code is for data subset where Investment class is BLEND\n",
    "Repeat the same set of actions as in VALUE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8217, 86)\n",
      "(2081, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "## Fulll set of code for Blend\n",
    "df_Blend_train = df_Blend[df_Blend.greatstone_rating.notnull()]\n",
    "df_Blend_test = df_Blend[df_Blend.greatstone_rating.isnull()]\n",
    "print(df_Blend_train.shape)\n",
    "print(df_Blend_test.shape)\n",
    "\n",
    "## 'New Code'\n",
    "df_linkage = df_dataset.filter(['fund_id', 'tag'],axis=1)\n",
    "\n",
    "df_linkage_Blend = df_Blend.filter(['fund_id', 'tag'],axis=1)\n",
    "\n",
    "df_Blend_train_keys = df_Blend_train.filter(['tag'],axis=1)\n",
    "df_Blend_test_keys = df_Blend_test.filter(['tag'],axis=1)\n",
    "\n",
    "keys_train = list(df_Blend_train_keys.columns.values)\n",
    "i1 = df_linkage_Blend.set_index(keys_train).index\n",
    "i2 = df_Blend_train_keys.set_index(keys_train).index\n",
    "df_Blend_linkage_train = df_linkage_Blend[i1.isin(i2)]\n",
    "df_Blend_linkage_test = df_linkage_Blend[~i1.isin(i2)]\n",
    "\n",
    "\n",
    "df_Blend_train.drop(columns=['bb_rating','below_b_rating','others_rating','maturity_bond','b_rating','a_rating','aaa_rating','aa_rating','bbb_rating','duration_bond'],inplace=True)\n",
    "df_Blend_test.drop(columns=['bb_rating','below_b_rating','others_rating','maturity_bond','b_rating','a_rating','aaa_rating','aa_rating','bbb_rating','duration_bond'],inplace=True)\n",
    "\n",
    "df_Blend_train_X = df_Blend_train.drop(labels='greatstone_rating',axis=1)\n",
    "df_Blend_train_y = df_Blend_train['greatstone_rating']\n",
    "\n",
    "## Below code for creatign train-test out of  Blend_Train\n",
    "BX_train, BX_test, By_train, By_test = train_test_split(df_Blend_train_X,df_Blend_train_y,test_size=0.2, random_state=108)\n",
    "\n",
    "# Drop funid & tag columns in both test & train \n",
    "df_Blend_train_X.drop(columns=['fund_id','tag'],inplace=True)\n",
    "df_Blend_test_y = df_Blend_test['greatstone_rating']\n",
    "df_Blend_test.drop(columns=['fund_id','tag','greatstone_rating'],inplace=True)\n",
    "\n",
    "# OneHot encode investment class in both train_X & test\n",
    "inv_class = pd.get_dummies(df_Blend_train_X['investment_class'],drop_first=True)\n",
    "df_Blend_train_X.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Blend_train_X = pd.concat([df_Blend_train_X,inv_class],axis=1)\n",
    "\n",
    "inv_class = pd.get_dummies(df_Blend_test['investment_class'],drop_first=True)\n",
    "df_Blend_test.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Blend_test = pd.concat([df_Blend_test,inv_class],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temp Model for Blend : (RAMDOM FOREST)\n",
    "\n",
    "## VX_train, VX_test, Vy_train, Vy_test\n",
    "BX_train.drop(columns=['fund_id','tag','investment_class'],inplace=True)\n",
    "BX_test.drop(columns=['fund_id','tag','investment_class'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       143\n",
      "         1.0       0.89      0.62      0.74       120\n",
      "         2.0       0.77      0.77      0.77       337\n",
      "         3.0       0.82      0.88      0.85       588\n",
      "         4.0       0.83      0.82      0.83       359\n",
      "         5.0       0.89      0.77      0.83        97\n",
      "\n",
      "    accuracy                           0.83      1644\n",
      "   macro avg       0.86      0.81      0.83      1644\n",
      "weighted avg       0.83      0.83      0.83      1644\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[143,   0,   0,   0,   0,   0],\n",
       "       [  0,  75,  44,   1,   0,   0],\n",
       "       [  3,   9, 260,  63,   2,   0],\n",
       "       [  0,   0,  32, 517,  38,   1],\n",
       "       [  0,   0,   2,  53, 296,   8],\n",
       "       [  0,   0,   0,   0,  22,  75]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=1500, max_depth=80, min_samples_split=5, min_samples_leaf=2, max_features = 'sqrt', bootstrap = False, random_state= 108)\n",
    "rf_model.fit(BX_train,By_train)\n",
    "predictions1 = rf_model.predict(BX_test)\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(By_test, predictions1)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(By_test, predictions1))\n",
    "\n",
    "metrics.confusion_matrix(By_test, predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98       143\n",
      "         1.0       0.90      0.67      0.77       120\n",
      "         2.0       0.75      0.78      0.76       337\n",
      "         3.0       0.83      0.83      0.83       588\n",
      "         4.0       0.80      0.83      0.82       359\n",
      "         5.0       0.84      0.78      0.81        97\n",
      "\n",
      "    accuracy                           0.82      1644\n",
      "   macro avg       0.84      0.81      0.83      1644\n",
      "weighted avg       0.82      0.82      0.82      1644\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[142,   0,   1,   0,   0,   0],\n",
       "       [  0,  80,  40,   0,   0,   0],\n",
       "       [  6,   9, 263,  57,   2,   0],\n",
       "       [  0,   0,  47, 488,  51,   2],\n",
       "       [  0,   0,   2,  45, 299,  13],\n",
       "       [  0,   0,   0,   0,  21,  76]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use XGBOOST\n",
    "D_train = xgb.DMatrix(BX_train, label=By_train)\n",
    "D_test = xgb.DMatrix(BX_test, label=By_test)\n",
    "\n",
    "param = {\n",
    "    'eta': 0.01,\n",
    "    'gamma': 1.75,\n",
    "    'min_child_weight': 9,\n",
    "    'max_depth': 5,  \n",
    "    'subsample': 1.0,\n",
    "    'random_state': 108,\n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 6} \n",
    "\n",
    "\n",
    "steps = 3000  # The number of training iterations\n",
    "\n",
    "xgb_model = xgb.train(param, D_train, steps)\n",
    "predictions = xgb_model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(By_test, best_preds)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(By_test, best_preds))\n",
    "\n",
    "metrics.confusion_matrix(By_test, best_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98       143\n",
      "         1.0       0.91      0.72      0.80       120\n",
      "         2.0       0.76      0.77      0.77       337\n",
      "         3.0       0.82      0.85      0.83       588\n",
      "         4.0       0.82      0.82      0.82       359\n",
      "         5.0       0.83      0.79      0.81        97\n",
      "\n",
      "    accuracy                           0.82      1644\n",
      "   macro avg       0.85      0.82      0.83      1644\n",
      "weighted avg       0.83      0.82      0.82      1644\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[142,   0,   1,   0,   0,   0],\n",
       "       [  0,  86,  34,   0,   0,   0],\n",
       "       [  6,   9, 261,  58,   3,   0],\n",
       "       [  0,   0,  46, 497,  43,   2],\n",
       "       [  0,   0,   1,  51, 293,  14],\n",
       "       [  0,   0,   0,   0,  20,  77]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1=XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=1.75,\n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
    "              min_child_weight=9, monotone_constraints=None,\n",
    "              n_estimators=3000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              objective='multi:softmax', random_state=108, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=None, silent=True, subsample=1.0,\n",
    "              tree_method=None, validate_parameters=False, verbosity=None)\n",
    "xgb1.fit(BX_train,By_train)\n",
    "predictions = xgb1.predict(BX_test)\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(By_test, predictions)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(By_test, predictions))\n",
    "\n",
    "metrics.confusion_matrix(By_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Below code is for data subset where Investment class is GROWTH\n",
    "Repeat the same set of actions as in VALUE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5328, 86)\n",
      "(1343, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "## Fulll set of code for Growth\n",
    "df_Growth_train = df_Growth[df_Growth.greatstone_rating.notnull()]\n",
    "df_Growth_test = df_Growth[df_Growth.greatstone_rating.isnull()]\n",
    "print(df_Growth_train.shape)\n",
    "print(df_Growth_test.shape)\n",
    "\n",
    "## 'New Code'\n",
    "df_linkage = df_dataset.filter(['fund_id', 'tag'],axis=1)\n",
    "\n",
    "df_linkage_Growth = df_Growth.filter(['fund_id', 'tag'],axis=1)\n",
    "\n",
    "df_Growth_train_keys = df_Growth_train.filter(['tag'],axis=1)\n",
    "df_Growth_test_keys = df_Growth_test.filter(['tag'],axis=1)\n",
    "\n",
    "keys_train = list(df_Growth_train_keys.columns.values)\n",
    "i1 = df_linkage_Growth.set_index(keys_train).index\n",
    "i2 = df_Growth_train_keys.set_index(keys_train).index\n",
    "df_Growth_linkage_train = df_linkage_Growth[i1.isin(i2)]\n",
    "df_Growth_linkage_test = df_linkage_Growth[~i1.isin(i2)]\n",
    "\n",
    "df_Growth_train.drop(columns=['bb_rating','below_b_rating','others_rating','maturity_bond','b_rating','a_rating','aaa_rating','aa_rating','bbb_rating','duration_bond'],inplace=True)\n",
    "df_Growth_test.drop(columns=['bb_rating','below_b_rating','others_rating','maturity_bond','b_rating','a_rating','aaa_rating','aa_rating','bbb_rating','duration_bond'],inplace=True)\n",
    "\n",
    "df_Growth_train_X = df_Growth_train.drop(labels='greatstone_rating',axis=1)\n",
    "df_Growth_train_y = df_Growth_train['greatstone_rating']\n",
    "\n",
    "## Below code for creatign train-test out of  Blend_Train\n",
    "GX_train, GX_test, Gy_train, Gy_test = train_test_split(df_Growth_train_X,df_Growth_train_y,test_size=0.2, random_state=108)\n",
    "\n",
    "\n",
    "# Drop funid & tag columns in both test & train \n",
    "df_Growth_train_X.drop(columns=['fund_id','tag'],inplace=True)\n",
    "df_Growth_test_y = df_Growth_test['greatstone_rating']\n",
    "df_Growth_test.drop(columns=['fund_id','tag','greatstone_rating'],inplace=True)\n",
    "\n",
    "# OneHot encode investment class in both train_X & test\n",
    "inv_class = pd.get_dummies(df_Growth_train_X['investment_class'],drop_first=True)\n",
    "df_Growth_train_X.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Growth_train_X = pd.concat([df_Growth_train_X,inv_class],axis=1)\n",
    "\n",
    "inv_class = pd.get_dummies(df_Growth_test['investment_class'],drop_first=True)\n",
    "df_Growth_test.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Growth_test = pd.concat([df_Growth_test,inv_class],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temp Model for Growth : (RAMDOM FOREST)\n",
    "\n",
    "## VX_train, VX_test, Vy_train, Vy_test\n",
    "GX_train.drop(columns=['fund_id','tag','investment_class'],inplace=True)\n",
    "GX_test.drop(columns=['fund_id','tag','investment_class'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97        61\n",
      "         1.0       0.81      0.72      0.76        54\n",
      "         2.0       0.83      0.84      0.83       197\n",
      "         3.0       0.87      0.87      0.87       358\n",
      "         4.0       0.80      0.84      0.82       274\n",
      "         5.0       0.84      0.80      0.82       122\n",
      "\n",
      "    accuracy                           0.84      1066\n",
      "   macro avg       0.86      0.84      0.85      1066\n",
      "weighted avg       0.85      0.84      0.84      1066\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 58,   0,   0,   3,   0,   0],\n",
       "       [  0,  39,  14,   1,   0,   0],\n",
       "       [  0,   8, 165,  22,   2,   0],\n",
       "       [  0,   0,  19, 310,  29,   0],\n",
       "       [  0,   1,   1,  22, 231,  19],\n",
       "       [  0,   0,   0,   0,  25,  97]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=500, max_depth=90, min_samples_split=10, min_samples_leaf=1, max_features = 'auto', bootstrap = False, random_state= 108)\n",
    "rf_model.fit(GX_train,Gy_train)\n",
    "predictions2 = rf_model.predict(GX_test)\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Gy_test, predictions2)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Gy_test, predictions2))\n",
    "\n",
    "metrics.confusion_matrix(Gy_test, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.97        61\n",
      "         1.0       0.81      0.72      0.76        54\n",
      "         2.0       0.84      0.83      0.83       197\n",
      "         3.0       0.86      0.86      0.86       358\n",
      "         4.0       0.80      0.85      0.83       274\n",
      "         5.0       0.86      0.81      0.84       122\n",
      "\n",
      "    accuracy                           0.84      1066\n",
      "   macro avg       0.86      0.84      0.85      1066\n",
      "weighted avg       0.85      0.84      0.84      1066\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 58,   0,   0,   2,   1,   0],\n",
       "       [  0,  39,  14,   1,   0,   0],\n",
       "       [  0,   6, 163,  26,   1,   1],\n",
       "       [  0,   2,  16, 307,  33,   0],\n",
       "       [  1,   1,   1,  22, 234,  15],\n",
       "       [  0,   0,   0,   1,  22,  99]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use XGBOOST\n",
    "D_train = xgb.DMatrix(GX_train, label=Gy_train)\n",
    "D_test = xgb.DMatrix(GX_test, label=Gy_test)\n",
    "\n",
    "param = {\n",
    "    'eta': 0.01, \n",
    "    'max_depth': 6,\n",
    "    'gamma': 1.5,\n",
    "    'min_child_weight':5,\n",
    "    'subsample': 0.8,\n",
    "    'random_state' : 108,\n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 6} \n",
    "\n",
    "\n",
    "steps = 3000  # The number of training iterations\n",
    "\n",
    "xgb_model = xgb.train(param, D_train, steps)\n",
    "predictions = xgb_model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Gy_test, best_preds)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Gy_test, best_preds))\n",
    "\n",
    "metrics.confusion_matrix(Gy_test, best_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.97        61\n",
      "         1.0       0.81      0.72      0.76        54\n",
      "         2.0       0.82      0.83      0.83       197\n",
      "         3.0       0.86      0.85      0.85       358\n",
      "         4.0       0.80      0.86      0.83       274\n",
      "         5.0       0.87      0.80      0.83       122\n",
      "\n",
      "    accuracy                           0.84      1066\n",
      "   macro avg       0.86      0.83      0.84      1066\n",
      "weighted avg       0.84      0.84      0.84      1066\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 58,   0,   0,   2,   1,   0],\n",
       "       [  0,  39,  15,   0,   0,   0],\n",
       "       [  0,   6, 163,  26,   1,   1],\n",
       "       [  0,   2,  19, 303,  34,   0],\n",
       "       [  1,   1,   1,  22, 236,  13],\n",
       "       [  0,   0,   0,   1,  24,  97]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1=XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1.0, gamma=1.5,\n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=5, monotone_constraints=None,\n",
    "              n_estimators=3000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=None, silent=True, subsample=0.8,\n",
    "              tree_method=None, validate_parameters=False, verbosity=None)\n",
    "\n",
    "xgb1.fit(GX_train,Gy_train)\n",
    "predictions = xgb1.predict(GX_test)\n",
    "#best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Gy_test, predictions)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Gy_test, predictions))\n",
    "\n",
    "metrics.confusion_matrix(Gy_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Below code is for data subset where Investment class Unknown\n",
    "Repeat the same set of actions as in VALUE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1205, 86)\n",
      "(275, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "## Fulll set of code for Unknown\n",
    "df_Unknown_train = df_Unknown[df_Unknown.greatstone_rating.notnull()]\n",
    "df_Unknown_test = df_Unknown[df_Unknown.greatstone_rating.isnull()]\n",
    "print(df_Unknown_train.shape)\n",
    "print(df_Unknown_test.shape)\n",
    "\n",
    "## 'New Code'\n",
    "df_linkage = df_dataset.filter(['fund_id', 'tag'],axis=1)\n",
    "\n",
    "df_linkage_Unknown = df_Unknown.filter(['fund_id', 'tag'],axis=1)\n",
    "\n",
    "df_Unknown_train_keys = df_Unknown_train.filter(['tag'],axis=1)\n",
    "df_Unknown_test_keys = df_Unknown_test.filter(['tag'],axis=1)\n",
    "\n",
    "keys_train = list(df_Unknown_train_keys.columns.values)\n",
    "i1 = df_linkage_Unknown.set_index(keys_train).index\n",
    "i2 = df_Unknown_train_keys.set_index(keys_train).index\n",
    "df_Unknown_linkage_train = df_linkage_Unknown[i1.isin(i2)]\n",
    "df_Unknown_linkage_test = df_linkage_Unknown[~i1.isin(i2)]\n",
    "\n",
    "df_Unknown_train.drop(columns=['bb_rating','below_b_rating','others_rating','maturity_bond','b_rating','a_rating','aaa_rating','aa_rating','bbb_rating','duration_bond'],inplace=True)\n",
    "df_Unknown_test.drop(columns=['bb_rating','below_b_rating','others_rating','maturity_bond','b_rating','a_rating','aaa_rating','aa_rating','bbb_rating','duration_bond'],inplace=True)\n",
    "\n",
    "## Introducign (V1.1 --> Drop lAST 3 COLS )  [post score V1]\n",
    "df_Unknown_train.drop(columns=['Medium','Small','Unknown','pc_ratio'],inplace=True)\n",
    "df_Unknown_test.drop(columns=['Medium','Small','Unknown','pc_ratio'],inplace=True)\n",
    "\n",
    "\n",
    "df_Unknown_train_X = df_Unknown_train.drop(labels='greatstone_rating',axis=1)\n",
    "df_Unknown_train_y = df_Unknown_train['greatstone_rating']\n",
    "\n",
    "## Below code for creatign train-test out of  Blend_Train\n",
    "UX_train, UX_test, Uy_train, Uy_test = train_test_split(df_Unknown_train_X,df_Unknown_train_y,test_size=0.2, random_state=108)\n",
    "\n",
    "# Drop funid & tag columns in both test & train \n",
    "df_Unknown_train_X.drop(columns=['fund_id','tag'],inplace=True)\n",
    "df_Unknown_test_y = df_Unknown_test['greatstone_rating']\n",
    "df_Unknown_test.drop(columns=['fund_id','tag','greatstone_rating'],inplace=True)\n",
    "\n",
    "# OneHot encode investment class in both train_X & test\n",
    "inv_class = pd.get_dummies(df_Unknown_train_X['investment_class'],drop_first=True)\n",
    "df_Unknown_train_X.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Unknown_train_X = pd.concat([df_Unknown_train_X,inv_class],axis=1)\n",
    "\n",
    "inv_class = pd.get_dummies(df_Unknown_test['investment_class'],drop_first=True)\n",
    "df_Unknown_test.drop(['investment_class'],axis=1,inplace=True)\n",
    "df_Unknown_test = pd.concat([df_Unknown_test,inv_class],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temp Model for Unknown : (RAMDOM FOREST)\n",
    "\n",
    "## VX_train, VX_test, Vy_train, Vy_test\n",
    "UX_train.drop(columns=['fund_id','tag','investment_class'],inplace=True)\n",
    "UX_test.drop(columns=['fund_id','tag','investment_class'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98        45\n",
      "         1.0       0.84      0.72      0.78        29\n",
      "         2.0       0.69      0.73      0.71        48\n",
      "         3.0       0.67      0.69      0.68        52\n",
      "         4.0       0.65      0.75      0.70        40\n",
      "         5.0       0.90      0.67      0.77        27\n",
      "\n",
      "    accuracy                           0.76       241\n",
      "   macro avg       0.79      0.76      0.77       241\n",
      "weighted avg       0.77      0.76      0.77       241\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[44,  0,  0,  1,  0,  0],\n",
       "       [ 0, 21,  7,  1,  0,  0],\n",
       "       [ 0,  4, 35,  9,  0,  0],\n",
       "       [ 1,  0,  7, 36,  8,  0],\n",
       "       [ 0,  0,  2,  6, 30,  2],\n",
       "       [ 0,  0,  0,  1,  8, 18]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=5000, max_depth=100, min_samples_split=10, min_samples_leaf=1, max_features = 'sqrt', bootstrap = False, random_state= 108)\n",
    "rf_model.fit(UX_train,Uy_train)\n",
    "predictions3 = rf_model.predict(UX_test)\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Uy_test, predictions3)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Uy_test, predictions3))\n",
    "\n",
    "metrics.confusion_matrix(Uy_test, predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.98      0.96        45\n",
      "         1.0       0.76      0.66      0.70        29\n",
      "         2.0       0.62      0.60      0.61        48\n",
      "         3.0       0.58      0.60      0.59        52\n",
      "         4.0       0.71      0.72      0.72        40\n",
      "         5.0       0.79      0.81      0.80        27\n",
      "\n",
      "    accuracy                           0.72       241\n",
      "   macro avg       0.73      0.73      0.73       241\n",
      "weighted avg       0.72      0.72      0.72       241\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[44,  0,  0,  1,  0,  0],\n",
       "       [ 0, 19,  8,  1,  0,  1],\n",
       "       [ 1,  6, 29, 12,  0,  0],\n",
       "       [ 1,  0, 10, 31,  8,  2],\n",
       "       [ 1,  0,  0,  7, 29,  3],\n",
       "       [ 0,  0,  0,  1,  4, 22]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use XGBOOST\n",
    "D_train = xgb.DMatrix(UX_train, label=Uy_train)\n",
    "D_test = xgb.DMatrix(UX_test, label=Uy_test)\n",
    "\n",
    "param = {\n",
    "    'eta': 0.01, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 1.0,\n",
    "    'gamma': 1.5,\n",
    "    'min_child_weight': 5,\n",
    "    'random_state': 108,\n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 6} \n",
    "\n",
    "steps = 3000  # The number of training iterations\n",
    "\n",
    "xgb_model = xgb.train(param, D_train, steps)\n",
    "predictions = xgb_model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Uy_test, best_preds)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Uy_test, best_preds))\n",
    "\n",
    "metrics.confusion_matrix(Uy_test, best_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98        45\n",
      "         1.0       0.75      0.62      0.68        29\n",
      "         2.0       0.64      0.71      0.67        48\n",
      "         3.0       0.60      0.62      0.61        52\n",
      "         4.0       0.69      0.68      0.68        40\n",
      "         5.0       0.78      0.78      0.78        27\n",
      "\n",
      "    accuracy                           0.73       241\n",
      "   macro avg       0.74      0.73      0.73       241\n",
      "weighted avg       0.73      0.73      0.73       241\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[44,  0,  0,  1,  0,  0],\n",
       "       [ 0, 18,  9,  1,  0,  1],\n",
       "       [ 0,  5, 34,  9,  0,  0],\n",
       "       [ 1,  0, 10, 32,  7,  2],\n",
       "       [ 0,  1,  0,  9, 27,  3],\n",
       "       [ 0,  0,  0,  1,  5, 21]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1=XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=1.5,\n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=9,\n",
    "              min_child_weight=5, monotone_constraints=None,\n",
    "              n_estimators=3000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=None, silent=True, subsample=1.0,\n",
    "              tree_method=None, validate_parameters=False, verbosity=None)\n",
    "\n",
    "xgb1.fit(UX_train,Uy_train)\n",
    "predictions = xgb1.predict(UX_test)\n",
    "#best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(Uy_test, predictions)\n",
    "##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "print(classification_report(Uy_test, predictions))\n",
    "\n",
    "metrics.confusion_matrix(Uy_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Below code is the final run :  \n",
    "Code below for each dataset on the initial train data (train + validation used earlier). Then predict using the test data. Finally, all the predictions (from 4 data subsets) will be merged & sorted as per the order in the  sample submission file. Finally the sample submission file will be created. \n",
    "\n",
    "#### Note that the choice of the algorithm, ( RF / XGBoost / XGB Classifier) depends on the accuracy durign the initial training pipeline. \n",
    "However, the code for all algos are given just in case needed, so that the final output can be based on choice -- just need to uncomment & run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investment class VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN\n",
    "# ## MODEL 3 : RAMDOM FOREST\n",
    "\n",
    "# rf_model = RandomForestClassifier(n_estimators=1000, max_depth=50, min_samples_split=5, min_samples_leaf=2, max_features = 'auto', bootstrap = False, random_state= 108)\n",
    "# rf_model.fit(df_Value_train_X,df_Value_train_y)\n",
    "# predictions = rf_model.predict(df_Value_test)\n",
    "\n",
    "# Value_predicted_ratings = pd.DataFrame(predictions)\n",
    "\n",
    "# df_Value_linkage_test.reset_index(drop=True, inplace=True)\n",
    "# Value_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "# df_Value_final_ratings = pd.concat([df_Value_linkage_test,Value_predicted_ratings],axis=1)\n",
    "# df_Value_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "# print(df_Value_final_ratings.shape)\n",
    "# df_Value_final_ratings.to_csv(\"df_Value_final_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1301, 2)\n"
     ]
    }
   ],
   "source": [
    "## MODEL 3 : XGB --> RUN THIS\n",
    "xgb1=XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, gamma=0.5, \n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=9,\n",
    "              min_child_weight=10, monotone_constraints=None,\n",
    "              n_estimators=3000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              objective='multi:softmax', random_state=108, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=None, silent=True, subsample=1.0,\n",
    "              tree_method=None, validate_parameters=False, verbosity=None)\n",
    "xgb1.fit(df_Value_train_X,df_Value_train_y)\n",
    "predictions = xgb1.predict(df_Value_test)\n",
    "\n",
    "\n",
    "Value_predicted_ratings = pd.DataFrame(predictions)\n",
    "\n",
    "\n",
    "df_Value_linkage_test.reset_index(drop=True, inplace=True)\n",
    "Value_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "df_Value_final_ratings = pd.concat([df_Value_linkage_test,Value_predicted_ratings],axis=1)\n",
    "df_Value_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "print(df_Value_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investment class BLEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2081, 2)\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS\n",
    "rf_model1 = RandomForestClassifier(n_estimators=1500, max_depth=80, min_samples_split=5, min_samples_leaf=2, max_features = 'sqrt', bootstrap = False, random_state= 108)\n",
    "rf_model1.fit(df_Blend_train_X,df_Blend_train_y)\n",
    "predictions1 = rf_model1.predict(df_Blend_test)\n",
    "\n",
    "Blend_predicted_ratings = pd.DataFrame(predictions1)\n",
    "\n",
    "df_Blend_linkage_test.reset_index(drop=True, inplace=True)\n",
    "Blend_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "df_Blend_final_ratings = pd.concat([df_Blend_linkage_test,Blend_predicted_ratings],axis=1)\n",
    "df_Blend_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "print(df_Blend_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_train = xgb.DMatrix(df_Blend_train_X, label=df_Blend_train_y)\n",
    "# D_test = xgb.DMatrix(df_Blend_test, label=df_Blend_test_y) \n",
    "\n",
    "# param = {\n",
    "#     'eta': 0.3, \n",
    "#     'max_depth': 3,  \n",
    "#     'objective': 'multi:softprob',  \n",
    "#     'num_class': 6} \n",
    "\n",
    "# steps = 1000  # The number of training iterations\n",
    "\n",
    "# xgb_model = xgb.train(param, D_train, steps)\n",
    "# predictions = xgb_model.predict(D_test)\n",
    "# best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "# Blend_predicted_ratings = pd.DataFrame(best_preds)\n",
    "\n",
    "# df_Blend_linkage_test.reset_index(drop=True, inplace=True)\n",
    "# Blend_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "# df_Blend_final_ratings = pd.concat([df_Blend_linkage_test,Blend_predicted_ratings],axis=1)\n",
    "# df_Blend_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "# print(df_Blend_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investment class Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1343, 2)\n"
     ]
    }
   ],
   "source": [
    "rf_model2 = RandomForestClassifier(n_estimators=500, max_depth=90, min_samples_split=10, min_samples_leaf=1, max_features = 'auto', bootstrap = False, random_state= 108)\n",
    "rf_model2.fit(df_Growth_train_X,df_Growth_train_y)\n",
    "predictions2 = rf_model2.predict(df_Growth_test)\n",
    "\n",
    "Growth_predicted_ratings = pd.DataFrame(predictions2)\n",
    "\n",
    "df_Growth_linkage_test.reset_index(drop=True, inplace=True)\n",
    "Growth_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "df_Growth_final_ratings = pd.concat([df_Growth_linkage_test,Growth_predicted_ratings],axis=1)\n",
    "df_Growth_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "print(df_Growth_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_train = xgb.DMatrix(df_Growth_train_X, label=df_Growth_train_y)\n",
    "# D_test = xgb.DMatrix(df_Growth_test, label=df_Growth_test_y) \n",
    "\n",
    "# param = {\n",
    "#     'eta': 0.3, \n",
    "#     'max_depth': 3,  \n",
    "#     'objective': 'multi:softprob',  \n",
    "#     'num_class': 6} \n",
    "\n",
    "# steps = 1000  # The number of training iterations\n",
    "\n",
    "# xgb_model = xgb.train(param, D_train, steps)\n",
    "# predictions = xgb_model.predict(D_test)\n",
    "# best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "# Growth_predicted_ratings = pd.DataFrame(best_preds)\n",
    "\n",
    "\n",
    "# df_Growth_linkage_test.reset_index(drop=True, inplace=True)\n",
    "# Growth_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "# df_Growth_final_ratings = pd.concat([df_Growth_linkage_test,Growth_predicted_ratings],axis=1)\n",
    "# df_Growth_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "# print(df_Growth_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investment class Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 2)\n"
     ]
    }
   ],
   "source": [
    "rf_model3 = RandomForestClassifier(n_estimators=5000, max_depth=100, min_samples_split=10, min_samples_leaf=1, max_features = 'sqrt', bootstrap = False, random_state= 108)\n",
    "rf_model3.fit(df_Unknown_train_X,df_Unknown_train_y)\n",
    "predictions3 = rf_model3.predict(df_Unknown_test)\n",
    "\n",
    "Unknown_predicted_ratings = pd.DataFrame(predictions3)\n",
    "\n",
    "df_Unknown_linkage_test.reset_index(drop=True, inplace=True)\n",
    "Unknown_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "df_Unknown_final_ratings = pd.concat([df_Unknown_linkage_test,Unknown_predicted_ratings],axis=1)\n",
    "df_Unknown_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "print(df_Unknown_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_train = xgb.DMatrix(df_Unknown_train_X, label=df_Unknown_train_y)\n",
    "# D_test = xgb.DMatrix(df_Unknown_test, label=df_Unknown_test_y) \n",
    "\n",
    "# param = {\n",
    "#     'eta': 0.3, \n",
    "#     'max_depth': 3,  \n",
    "#     'objective': 'multi:softprob',  \n",
    "#     'num_class': 6} \n",
    "\n",
    "# steps = 500  # The number of training iterations\n",
    "\n",
    "# xgb_model = xgb.train(param, D_train, steps)\n",
    "# predictions = xgb_model.predict(D_test)\n",
    "# best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "# Unknown_predicted_ratings = pd.DataFrame(best_preds)\n",
    "\n",
    "# df_Unknown_linkage_test.reset_index(drop=True, inplace=True)\n",
    "# Unknown_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "# df_Unknown_final_ratings = pd.concat([df_Unknown_linkage_test,Unknown_predicted_ratings],axis=1)\n",
    "# df_Unknown_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "# print(df_Unknown_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# xgb1=XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.7, gamma=1.5,\n",
    "#               importance_type='gain', interaction_constraints=None,\n",
    "#               learning_rate=0.01, max_delta_step=0, max_depth=9,\n",
    "#               min_child_weight=5, monotone_constraints=None,\n",
    "#               n_estimators=3000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "#               objective='multi:softprob', random_state=0, reg_alpha=0,\n",
    "#               reg_lambda=1, scale_pos_weight=None, silent=True, subsample=1.0,\n",
    "#               tree_method=None, validate_parameters=False, verbosity=None)\n",
    "# xgb1.fit(df_Unknown_train_X,df_Unknown_train_y)\n",
    "# predictions = xgb1.predict(df_Unknown_test)\n",
    "\n",
    "\n",
    "# Unknown_predicted_ratings = pd.DataFrame(predictions)\n",
    "\n",
    "# df_Unknown_linkage_test.reset_index(drop=True, inplace=True)\n",
    "# Unknown_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "# df_Unknown_final_ratings = pd.concat([df_Unknown_linkage_test,Unknown_predicted_ratings],axis=1)\n",
    "# df_Unknown_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "# print(df_Unknown_final_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge predictipons from all data subsets to create finbal set & order that based on the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fund_id</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468b76c0-a276-45b9-ad76-1a9d6c336ed6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd9b1f78-e450-489f-b6e5-ece691d5c021</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7723006e-720e-4c97-825f-1752cd112736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37c33326-b1fb-4208-a7df-08f7ead749da</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d1052147-38d7-4981-8b8c-b3a0dcda8aed</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>668a58e7-5bee-41fd-9e23-963aa2f08f01</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>96c3122c-eea0-4d39-b4ed-c1647b03f834</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>80001be5-fde4-4f20-a8c3-ec295aed5960</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>4d555bb4-ada5-4a1c-8a01-0827477dd7e5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>6b5b4d1a-f208-4a6b-88ce-1b42ed63ef2e</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  fund_id    0\n",
       "0    468b76c0-a276-45b9-ad76-1a9d6c336ed6  3.0\n",
       "1    cd9b1f78-e450-489f-b6e5-ece691d5c021  1.0\n",
       "2    7723006e-720e-4c97-825f-1752cd112736  0.0\n",
       "3    37c33326-b1fb-4208-a7df-08f7ead749da  2.0\n",
       "4    d1052147-38d7-4981-8b8c-b3a0dcda8aed  2.0\n",
       "..                                    ...  ...\n",
       "270  668a58e7-5bee-41fd-9e23-963aa2f08f01  3.0\n",
       "271  96c3122c-eea0-4d39-b4ed-c1647b03f834  0.0\n",
       "272  80001be5-fde4-4f20-a8c3-ec295aed5960  3.0\n",
       "273  4d555bb4-ada5-4a1c-8a01-0827477dd7e5  0.0\n",
       "274  6b5b4d1a-f208-4a6b-88ce-1b42ed63ef2e  2.0\n",
       "\n",
       "[275 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Value_final_ratings.rename(columns={\"0\": \"greatstone_rating\"})\n",
    "df_Growth_final_ratings.rename(columns={\"0\": \"greatstone_rating\"})\n",
    "df_Blend_final_ratings.rename(columns={\"0\": \"greatstone_rating\"})\n",
    "df_Unknown_final_ratings.rename(columns={\"0\": \"greatstone_rating\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_Value_final_ratings,df_Blend_final_ratings,df_Growth_final_ratings,df_Unknown_final_ratings]\n",
    "final_result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder linkage_test to match the submission file & Create the final file\n",
    "final_result = final_result.set_index('fund_id')\n",
    "final_result = final_result.reindex(index=df_submission_file['fund_id'])\n",
    "final_result = final_result.reset_index()\n",
    "final_result.to_csv(\"FinalSubmissionFile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After the fikle is created, open the csv file, delete the firsat column & rename the last column to 'greatstone_rating'\n",
    "## before submission  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF PROGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional : Code for using LGBM algo\n",
    "\n",
    "Note : You need to install Light GBM  using pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## For initial training phase\n",
    "# D_train = lgb.Dataset(VX_train, label=Vy_train)\n",
    "# D_test = lgb.Dataset(VX_test, label=Vy_test)\n",
    "\n",
    "# params = {}\n",
    "# params['learning_rate'] = 0.02\n",
    "# params['boosting_type'] = 'gbdt'\n",
    "# params['objective'] = 'multiclass'\n",
    "# params['metric'] = 'multi_logloss'\n",
    "# params['sub_feature'] = 0.5\n",
    "# params['num_leaves'] = 10\n",
    "# params['min_data'] = 50\n",
    "# params['max_depth'] = 9\n",
    "# #params['feature_fraction']=0.8\n",
    "# params['bagging_fraction']=0.6\n",
    "# params['num_boost_round']=6000\n",
    "# params['random_state'] = 108\n",
    "# params['num_class']= 6\n",
    "\n",
    "# steps =6000\n",
    "\n",
    "# clf = lgb.train(params, D_train, steps)\n",
    "\n",
    "# predictions=clf.predict(VX_test)\n",
    "\n",
    "# best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "\n",
    "# accuracy_score = metrics.accuracy_score(Vy_test, best_preds)\n",
    "# ##print(\"Accuracy score is \" + str(accuracy_score))\n",
    "\n",
    "# print(classification_report(Vy_test, best_preds))\n",
    "\n",
    "# metrics.confusion_matrix(Vy_test, best_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## For the final run\n",
    "# D_train = lgb.Dataset(df_Value_train_X, label=df_Value_train_y)\n",
    "# D_test = lgb.Dataset(df_Value_test, label=df_Value_test_y)\n",
    "\n",
    "# params = {}\n",
    "# params['learning_rate'] = 0.02\n",
    "# params['boosting_type'] = 'gbdt'\n",
    "# params['objective'] = 'multiclass'\n",
    "# params['metric'] = 'multi_logloss'\n",
    "# params['sub_feature'] = 0.5\n",
    "# params['num_leaves'] = 10\n",
    "# params['min_data'] = 50\n",
    "# params['max_depth'] = 5\n",
    "# #params['feature_fraction']=0.8\n",
    "# params['bagging_fraction']=0.6\n",
    "# params['num_boost_round']=6000\n",
    "# params['random_state'] = 108\n",
    "# params['num_class']= 6\n",
    "\n",
    "# steps =6000\n",
    "\n",
    "\n",
    "# clf = lgb.train(params, D_train, steps)\n",
    "\n",
    "# predictions=clf.predict(df_Value_test)\n",
    "\n",
    "# best_preds = np.asarray([np.argmax(line) for line in predictions])\n",
    "\n",
    "# Growth_predicted_ratings = pd.DataFrame(best_preds)\n",
    "\n",
    "# df_Value_linkage_test.reset_index(drop=True, inplace=True)\n",
    "# Value_predicted_ratings.reset_index(drop=True, inplace=True)\n",
    "# df_Value_final_ratings = pd.concat([df_Value_linkage_test,Value_predicted_ratings],axis=1)\n",
    "# df_Value_final_ratings.drop(columns=['tag'],inplace=True)\n",
    "# print(df_Value_final_ratings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
